{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: \n",
    "\n",
    "**DUE:** 5pm EST, Feb 27, 2020\n",
    "\n",
    "I completed this homework together with Shirley, Doug, Debbie, Nabila, and Brett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conceptual:** Short answer questions. Be concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Using the mean squared error (MSE) as your objective function allows for a closed form solution to finding the maximum likelihood estimate (MLE) of your model parameters in linear regression. Let’s consider the simple, single predictor variable model $Y= \\beta_0 + \\beta_1 X $. \n",
    "\n",
    "a) Use algebra to show how you can expand out the MSE to get from i to ii below. Do your proof using Latex equation formatting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _i)_ $MSE(\\beta_0, \\beta_1)=E[ (Y-(\\beta_0 + \\beta_1 X))^2]$ \n",
    " \n",
    " _ii)_ $MSE(\\beta_0, \\beta_1)=E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Note that $E(XY) = Cov[X,Y] + E(X)E(Y)$ and $E(X^2) = Var(x) + E(X)^2)$\n",
    "\n",
    "\n",
    "1) $MSE(\\beta_0, \\beta_1)=E[ (Y-(\\beta_0 + \\beta_1 X))^2]$ \n",
    "\n",
    "2) $MSE(\\beta_0, \\beta_1)=E[ (Y-\\beta_0 - \\beta_1 X)(Y-\\beta_0 - \\beta_1 X)]$\n",
    "\n",
    "3) $MSE(\\beta_0, \\beta_1)= E(Y^2) - \\beta_0E(Y) - \\beta_1E(XY) -\\beta_0E(Y) + \\beta_0^2 + \\beta_0\\beta_1E(X) - \\beta_1E(XY) + \\beta_0\\beta_1E(X) + \\beta_1^2E(X^2)$\n",
    "\n",
    "4) $MSE(\\beta_0, \\beta_1)= E(Y^2) - 2\\beta_0E(Y) - 2\\beta_1E(XY) + \\beta_0^2 + 2\\beta_0\\beta_1E(X) + \\beta_1^2E(X^2)$\n",
    "\n",
    "5)  $MSE(\\beta_0, \\beta_1)= E(Y^2) - 2\\beta_0E(Y) - 2\\beta_1Cov[X,Y] -2\\beta_1E(X)E(Y)) + \\beta_0^2 + 2\\beta_0\\beta_1E(X) + \\beta_1^2Var(X) + \\beta_1^2(E(X))^2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Prove that the MLE of $\\beta_0$ is $E[Y]- \\beta_1 E[X]$ by taking the derivative of _ii_ above, with respect to $\\beta_0$, setting the derivative to zero, and solving for $\\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "$MSE(\\beta_0, \\beta_1)=E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$\n",
    "\n",
    "$\\frac{\\partial MSE}{\\partial \\beta_0} :   -2\\beta_0E(Y) + \\beta_0^2 + 2\\beta_0\\beta_1E(X)$\n",
    "\n",
    "$ 0 = -2E(Y) + 2\\beta_0 + 2\\beta_1E(X)$\n",
    "\n",
    "$ 0 = -E(Y) + \\beta_0 + \\beta_1E(X)$\n",
    "\n",
    "$-\\beta_0 = -E(Y) + \\beta_1E(X)$\n",
    "\n",
    "$\\beta_0 = E(Y) - \\beta_1E(X)$\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Prove that the MLE for $\\beta_1$ is $Cov[X,Y]/Var[X]$ by taking the derivative of _ii_ above, with respect to $\\beta_1$, setting the derivative to zero, and solving for $\\beta_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "$MSE(\\beta_0, \\beta_1)=E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$\n",
    "\n",
    "$\\frac{\\partial MSE}{\\partial \\beta_1} :    -2\\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y] +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$\n",
    "\n",
    "$ 0 = -2Cov[X,Y] -2E(X)E(Y) + 2\\beta_0E(X) + 2\\beta_1Var(X) + 2\\beta_1E(X)^2$\n",
    "\n",
    "$0 = -Cov[X,Y] -E(X)E(Y) + \\beta_0E(X) + \\beta_1Var(X) + \\beta_1E(X)^2$\n",
    "\n",
    "$0 = -Cov[X,Y] -E(X)E(Y) + E(Y)E(X) - \\beta_1E(X)^2 + \\beta_1Var(X) + \\beta_1E(X)^2$\n",
    "\n",
    "$0 = -Cov[X,Y]  + \\beta_1Var(X) $\n",
    "\n",
    "$\\beta_1Var(X) = Cov[X,Y] $\n",
    "\n",
    "$\\beta_1 = \\frac{Cov[X,Y]}{Var(X)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2. Using the MLE solutions shown in 1(b) and 1(c) above, show how, in the case of simple linear regression, the least squares line always passes through the point $(E(X), E(Y))$ or $(x,y)$). What does this imply about the leverage that the response and predictor variable means have on the ordinary least squares solution?\n",
    "\n",
    "note that $\\beta_0 = E(Y) - \\beta_1E(X)$\n",
    "\n",
    "$E(Y) = \\beta_0 + \\beta_1E(X)$\n",
    "\n",
    "$E(Y) = E(Y) - \\beta_1E(X) + \\beta_1E(X)$\n",
    "\n",
    "$E(Y) = E(Y)$\n",
    "\n",
    "If the regression line always passes through the mean of X and Y, then any point far from the mean will influence the regression line by shifting the mean. The farther the point from the mean, the more the regession line will be shifted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. In the case where X and Y are standard normal variables (i.e. $N(0,1)$), show that the ordinary least squares solution converges to the same solution as the Pearson correlation coefficient $r=Cov[X,Y]/(\\sigma_X * \\sigma_Y)$. What exactly do they both converge to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "$r=\\frac{Cov[X,Y]}{(\\sigma_X * \\sigma_Y)}$\n",
    "\n",
    "$\\beta_1 =  \\frac{Cov(X,Y)}{Var(X)}$\n",
    "\n",
    "$\\beta_1 =  \\frac{Cov(X,Y)}{\\sigma_X^2}$\n",
    "\n",
    "$\\beta_1 =  \\frac{Cov(X,Y)}{\\sigma_X *\\sigma_X}$\n",
    "\n",
    "$\\beta_1 =  \\frac{Cov(X,Y)}{\\sigma_X * \\sigma_X} * \\frac{\\sigma_Y} {\\sigma_Y}$\n",
    "\n",
    "$\\beta_1 =  \\frac{Cov(X,Y)}{\\sigma_X * \\sigma_Y} * \\frac{\\sigma_Y} {\\sigma_X}$\n",
    "\n",
    "$\\beta_1\\frac{\\sigma_X}{\\sigma_Y} = \\frac{Cov[X,Y]}{(\\sigma_X * \\sigma_Y)} = r$\n",
    "\n",
    "In the case where X and Y are standard variables, meaning they have the same standard deviation ($\\frac{\\sigma_X}{\\sigma_Y}=1$) then the regression coefficient ($\\beta_1$) for X is the same as r. Conceptually, they both indicate the change in Y in standard deviation units for every one standard deviation increase in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4. Three methods for evaluating a regression model’s performance are $r^2$, $RSE$, and the $F$ Test. Show the functional form of each method and describe conceptually what it is telling you about your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "\n",
    "$r^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "$r^2$ tells us the percentage of variance in Y that is explained by X. TSS is the overall variance in Y whereas RSS is the residuals from the model (or the variance left over once X has accounted for variance in Y). The form shows us that as the variance explained by X increases, thus making the residuals smaller and the numerator of the fraction smaller, the ratio of RSS:TSS becomes smaller. $r^2$ is a proportion of variance (ranging from 0-1) and thus $r^2$ increases as the ratio of RSS:TSS decreases. Conceptually this tells us that as $r^2$ increases, our model can predict a greater proportion of variance in Y relative to all the leftover, or unexplained, variance in Y.\n",
    "\n",
    "$RSE = \\sqrt{\\frac{RSS}{n-2}}$\n",
    "\n",
    "RSE (or the residual standard error) functions like a compliment to $r^2$, telling us a  estimate of the variance of the residual error in the model, or the variance not explained by X. RSE tells us the average amount that the observed data points deviate from the expected values on the regression line. The RSE is a function of the error of the model over n-2, which reflects the sample size. This means that the larger the ratio of error:n (either with less error or a larger sample size), the smaller the RSE, or the better the model fits the data.\n",
    "\n",
    "$F = \\frac{\\frac{(TSS-RSS)}{p}}{\\frac{RSS}{n-p-1}}$\n",
    "\n",
    "The F statistic is a proportion of the variance explained by X (TSS-RSS) as a function of the number of predictors over the variance not explained by X (RSS) as a function of the sample size minus the necessary degrees of freedom. The F statistic essentially tells us the proportion of explained variance to unexplained variance, accounting for sample size and number of predictors. As the proportion of variance explained by the model increases relative to the unexplained variance and the number of parameters in the model, and/or as the sample size increases, the F statistic will increase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Applied:** Show your code & plots\n",
    "\n",
    "We will use the HCP dataset (see Homework 2) for these problems. Include conceptual answers to questions as comments in code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Here we will look at how different measures including gender, gray matter volume, and white matter volume can associate with the unadjusted flanker task score \n",
    "\n",
    "(a) Load the HCP data set and subset the data by selecting only the Subject, Gender, Flanker-Unadj, FS_Tot_WM_Vol and FS_Total_GM_Vol columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Question 5(a)\n",
    "# -------------------------------\n",
    "\n",
    "unrestricted_trimmed_1_7_2020_10_50_44 <- read.csv(\"~/GitHub_Repo/DataScience2020/Maheux_DSPN_S20/Data/unrestricted_trimmed_1_7_2020_10_50_44.csv\", header=TRUE)\n",
    "data <- unrestricted_trimmed_1_7_2020_10_50_44 \n",
    "\n",
    "library(tidyverse)\n",
    "\n",
    "data$Flanker <- data$Flanker_Unadj\n",
    "data %>% select(Subject, Gender, Flanker, FS_Total_GM_Vol, FS_Tot_WM_Vol, Age, Gender) -> dat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Fit a simple linear regression model with unadjusted Flanker task scores as the repsonse variable (i.e., Y) and total grey matter volume as the predictor variable (i.e., X). Show the summary of the model, extract the coefficients and the confidence intervals, and report what inference you would make based on these numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Question 5(b)\n",
    "# -------------------------------\n",
    "model1 <- lm(Flanker~FS_Total_GM_Vol, data=dat)\n",
    "summary(model1)\n",
    "confint(model1)\n",
    "\n",
    "#the coefficient for the grey matter volume is 3.110e-05 and the intercept is 9.026e+01. \n",
    "#The confidence intervals are shown below: both intervals are very small and neither contain 0.\n",
    "#based on these numbers, I can infer that there is a significant positive association between grey matter volume and scores on the Flanker task, though the effect seems to be rather small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Generate all the residual plots to check whether the fit model meets our assumptions. Then generate a scatter plot with *ggplot*, using *geom_smooth*, to show the residuals and check our assumptions. Does it look like the data is meeting the assumptions of linear regression. Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Question 5(c)\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "plot(model1)\n",
    "\n",
    "ggplot(dat, aes(x = FS_Total_GM_Vol, y = Flanker)) +\n",
    "  geom_point() + geom_smooth(method = \"lm\")\n",
    "\n",
    "#yes, it does look like all the assumptions of linear regression are being met. \n",
    "#the QQ plot shows that Y is normally distributed. Deviations from the straight line, indicating the normal distribution, are minimal.\n",
    "#the regression line plot shows visually that F(X) is linear (there is only one predicted value of y for each value of x)\n",
    "#there is only one x variable, so collinearity is not a concern\n",
    "#The residual plot shows that f(x) is stationary, such that all data points appear visually to be sampled from the same distribution, rather than showing discrete groups of data points.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. Here we will ask about the same associations between the grey matter volume and Flanker task performance, but from a prediction perspective. Let's say you collected more data for 5 more participants and you get their total grey matter volume as follows: 612432, 702562, 596433, 695982, 649230. Predict what their unadjusted Flanker score is going to be based on the model that we just fit. Store all the values and CI bounds in a data frame called *predicted_table* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Question 6\n",
    "# -------------------------------\n",
    "\n",
    "predicted_table <- data.frame(FS_Total_GM_Vol=c(612432, 702562, 596433, 695982, 649230))\n",
    "predicted_table$Flanker <- predict(model1, newdata=predicted_table, interval='confidence')\n",
    "print(predicted_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Now we can explore main effects and interactions\n",
    "\n",
    "(a) Fit a multiple regression model with the unadjusted Flanker scores as the response variable and white matter volume, grey matter volume, and gender included as main effect terms (i.e., predictor variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Question 7(a)\n",
    "# -------------------------------\n",
    "#\n",
    "model2 <- lm(Flanker~FS_Total_GM_Vol + FS_Tot_WM_Vol + Gender, data=dat)\n",
    "summary(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Now update the model with gender and white matter volume removed. What has changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Question 7(b)\n",
    "# -------------------------------\n",
    "model3 <- lm(Flanker~FS_Total_GM_Vol, data=dat)\n",
    "summary(model3)\n",
    "\n",
    "#When removing gender and white matter as predictors, the model fit statistics change.\n",
    "#R^2 is slightly greater in the prior model, meaning that there is more variance explained with 3 predictors compared to 1.\n",
    "#R^2 always increases with increased predictors, however, so we cannot assume this is necessarily an indicator of better model fit.\n",
    "#The F stat is much greater in this more simple model, suggesting that this model fits the data better than the prior model with 3 predictors\n",
    "#The F stat takes into account number of predictors relative to sample size. The model statistics here show that the more simple model is preferred in terms of model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Construct a new model but add in a new interaction term for the interaction between gender and gray matter volume. Extract all of the coefficients for the model. What have we learned?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Question 7(c)\n",
    "# -------------------------------\n",
    "\n",
    "model4 <- lm(Flanker~FS_Total_GM_Vol*Gender + FS_Tot_WM_Vol , data=dat)\n",
    "summary(model4)\n",
    "\n",
    "#The coefficients show that the effect of grey matter volume on the Flanker task score does not depend on gender. Grey matter volume has a significant positive main effect on Flanker scores.\n",
    "#This model has slightly worse fit to the data than the prior model with 3 predictors that did not include the interaction term\n",
    "#R^2 is slighlty larger and the F statistic is slightly smaller for this model compared to the model with 3 predictors, likely because this model includes an additional term which artificially inflates R^2 and appropriately reduces the F stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
